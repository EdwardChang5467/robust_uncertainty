# 1. Introduction #

This repository contains the code implementation and relevant data for the paper titled "**Robust Uncertainty Quantification for Factual Generation of Large Language Models**," which was accepted at **IJCNN 2025**. It provides a pipeline implementation for generating trap questions in multi-fact generation for LLMs and a robust uncertainty quantification method **RU** for multi-fact generation that considers these traps. The proposed approach achieves an average improvement of 0.1-0.2 in ROCAUC values and correlation scores compared to baseline methods, offering new insights and directions for uncertainty quantification in multi-fact generation by LLMs.



# 2. Repository structure

- **`./cache`**: This directory is used to store intermediate results during the execution process, which can be helpful for error tracing and localization.
- **`./utils`**: This directory contains the code for generating and evaluating trap questions, as well as simple utility code that may be used by all the scripts in the repository.
- **`./code`**: This directory includes the implementation of the uncertainty quantification methods proposed in the paper, as well as the code for the baseline methods.
- **`./data`**: This directory stores the trap questions generated by the code, as well as the sampling and measurement results of the uncertainty quantification methods.



# 3. How to run the code #

Before running the code in this repository, you need to set up the environment first. We provide two configuration files, and you can use either one to configure your environment. 

The python version used in this repository is **Python 3.10.15**.

First, use `conda create -n your_env_name python=3.10.15 ` to create a Python environment.

Next, use the environment configuration file provided by the repository to install the repository code running environment.

For example, you can install the required packages for running the code by executing the following command:

```bash
pip install -r requirements.txt
```

If you choose to use the alternative configuration file, please refer to the corresponding information provided in the file.



## Trap questions generation

First, set up the CSV file containing real names that will be referenced when generating fake names. The format can be referred to from `./data/test.csv`.

After setting up the CSV file, configure the cache path, the API URL, and the key for the generation model in the `generate_name.py` file. Run the script to generate a list of fake names. The output will be saved as `{model_name}_fake_name_list.json` in the `./cache` directory.

Optionally, review the generated list of fake names and make fine-tuning adjustments. You can add manually constructed fake names or remove any that seem unsuitable.

To verify the generated fake names, run `check_name.py` for general verification or `check_name_2.py` for the verification using the Chain-of-Thought (CoT) strategy. Before running these scripts, set up the data storage path, the API URL and key for the verification model, and the sampling quantity. The verification results will be generated accordingly.

We have tested several high-performing models (GPT-4o, kimi, yi-lightning) on the fake name verification task. The average accuracy is approximately 0.8, and the average F1 score is around 0.85.

If you are not fully confident in the model's verification results, you can perform a manual search-based verification.

Finally, run `get_trap_questions.py` to obtain the set of trap questions for multi-fact generation.



## Uncertainty Quantification for multi-fact generation

Firstly, the model generation sampling is performed using the `generate.py` script. To achieve faster loading speeds, the current implementation utilizes local loading for the model. Depending on your specific requirements, you can modify the code to change the loading method (for example, directly loading from Huggingface or Github).

```bash
python generate.py 
--model_path YOUR_MODEL_PATH
--file_path YOUR_QUESTION_PATH
--model_name YOUR_MODEL_NAME
--number_of_generations 5
--num_beams 5
--top_p 1.0
--temperature 1.0
--cuda 0 (If you use GPU for sampling, add this parameter.)
```

Next, use the `seperate_facts.py` script to perform fact decomposition on the generated output. Before running the script, ensure that you have set up the model name, API, and key for fact decomposition.

```bash
python seperate_facts.py --file_path YOUR_GENERATION_PATH
```

Use the `get_classification.py` script to classify the generated content. You can either pre-specify lists of real and fake names or utilize a Large Language Model (LLM) to classify the generated content.

```bash
python get_classification.py 
--generation_path YOUR_FACT_FILE_PATH
--true_name_path YOUR_TRUE_NAME_PATH
--fake_name_path YOUR_FAKE_NAME_PATH
--output_path YOUR_OUTPUT_PATH
```

 Use the `get_mapping.py` script to perform token mapping.

```bash
python get_mapping.py
--file_path YOUR_CLASSIFICATION_FILE_PATH
--model_path YOUR_GENERATING_MODEL_PATH 
--output_path YOUR_OUTPUT_PATH
--cuda 0 (If you use GPU to load model, add this parameter.)
```

Use the `compute_uncertainty.py` script to calculate uncertainty.

```bash
python compute_uncertainty.py
--file_path YOUR_TOKEN_MAPPING_FILE_PATH
--output_path YOUR_OUTPUT_PATH
```



